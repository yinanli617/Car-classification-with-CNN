{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dabc7df-f8cf-4ab2-a4f4-446359460464",
   "metadata": {},
   "source": [
    "# Classification of cars using CNN and transfer learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this project, I will tackle a classification problem using the [stanford car dataset](https://ai.stanford.edu/~jkrause/cars/car_dataset.html). This dataset contains 16,185 images of 196 classes of cars. The data is split into 8,144 training images and 8,041 testing images, where each class has been split roughly in a 50-50 split. Classes are typically at the level of Make, Model, Year, e.g. 2012 Tesla Model S or 2012 BMW M3 coupe. **The objective of this project is to train a deep learning model that takes an image of a car as input and then output its class**.\n",
    "\n",
    "Due to the limited number of images, it is difficult to train a deep learning model from scratch with this dataset. Therefore I will leverage the power of **transfer learning**, a common approch used in deep learning (especially in computer vision) and use a pretrained efficientnet model and fine-tune it on our own dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ddaf4-ef49-4b13-8bc3-79b1bc9fbeae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab1c9a01-3aed-44d6-808c-34980e335e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import timm\n",
    "\n",
    "import time\n",
    "import os\n",
    "import tqdm\n",
    "import PIL.Image as Image\n",
    "from IPython.display import display\n",
    "import logging\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# print(torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61972e4f-04cb-45e5-9ab3-026b20afeed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = './stanford_car_dataset/car_data/car_data/'\n",
    "NUM_CAR_CLASSES = 196\n",
    "LOG_PATH = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f823fc-bc45-4e8d-b8fb-30ee9bfc6c09",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset is stored in the following structure:\n",
    "\n",
    "└── car_data\n",
    "\n",
    "    ├── test ── <car_class> ── images\n",
    "    \n",
    "    └── train ── <car_class> ── images\n",
    "    \n",
    "In other words, in each of the train/test directory, there are 196 subdirectories with unique identifiers (the name of the car class). In each of these subdirectories, images belonging to the corresponding car class are stored.\n",
    "\n",
    "Given the structure of this dataset, the convenient utility tool `torchvision.datasets.ImageFolder`, which is a subclass of `torch.utils.data.Dataset` will prove quite handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c72a1e3-f46d-4d13-8a0f-e80a4d42aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! tree -d ./stanford_car_dataset/car_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16b4c4b9-c775-43b5-9595-bb4cabd460f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(dataset_dir):\n",
    "    # To help prevent overfitting, I did some simple augmentation including horizontal flip and rotation here.\n",
    "    # For more image augmentation options, the albumentations package works really well with pytorch\n",
    "    # https://github.com/albumentations-team/albumentations\n",
    "\n",
    "    # On another note, I recently found another helpful package - torchIO which is good for 3d volume dataset\n",
    "    # https://github.com/fepegar/torchio\n",
    "\n",
    "    # note: no data augmentation for test data\n",
    "\n",
    "    width, height = 224, 224\n",
    "    train_tfms = transforms.Compose([transforms.Resize((width, height)),\n",
    "                                     transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.RandomRotation(15),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    test_tfms = transforms.Compose([transforms.Resize((width, height)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    # create datasets\n",
    "    train_dset = torchvision.datasets.ImageFolder(root=dataset_dir + \"train\", transform=train_tfms)\n",
    "    train_dl = torch.utils.data.DataLoader(train_dset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "    test_dset = torchvision.datasets.ImageFolder(root=dataset_dir + \"test\", transform = test_tfms)\n",
    "    test_dl = torch.utils.data.DataLoader(test_dset, batch_size=32, shuffle=False, num_workers=2)\n",
    "    \n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a869638f-7ab8-4b09-9d67-c887c48e62e5",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f52a59d-7315-466a-a5ec-6655b9f51846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_dl, loss_fn, optimizer, scheduler, n_epochs=5):\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    # set the model to train mode initially\n",
    "    model.train()\n",
    "    for epoch in tqdm.tqdm(range(1, n_epochs+1)):\n",
    "        since = time.time()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0.0\n",
    "        for i, data in enumerate(train_dl, 0):\n",
    "\n",
    "            # get the inputs and assign them to cuda\n",
    "            inputs, labels = data\n",
    "            #inputs = inputs.to(device).half() # uncomment for half precision model\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            predicted = F.softmax(outputs, dim=-1).argmax(dim=-1)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # calculate the loss/acc later\n",
    "            running_loss += loss.item() * inputs.shape[0]\n",
    "            running_correct += (labels==predicted).sum().item()\n",
    "\n",
    "        epoch_duration = time.time() - since\n",
    "        epoch_loss = running_loss / len(train_dl.dataset)\n",
    "        epoch_acc = running_correct / len(train_dl.dataset) * 100.0\n",
    "        msg = f\"Train Epoch: {epoch}\\tduration: {epoch_duration}s\\ttrain_loss: {epoch_loss:.4f}\\taccuracy: {epoch_acc:.4f}%\"\n",
    "        logging.info(msg)\n",
    "        \n",
    "        losses.append(epoch_loss)\n",
    "        accuracies.append(epoch_acc)\n",
    "        \n",
    "        # switch the model to eval mode to evaluate on test data\n",
    "        model.eval()\n",
    "        test_acc = eval_model(model)\n",
    "        test_accuracies.append(test_acc)\n",
    "        \n",
    "        # re-set the model to train mode after validating\n",
    "        model.train()\n",
    "        scheduler.step(test_acc)\n",
    "        since = time.time()\n",
    "    print('Finished Training')\n",
    "    return model, losses, accuracies, test_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efde05-5860-41d4-a022-cf339eb63aa2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa2add87-12c2-4e94-9c2d-9ffc3d7d054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_dl, 0):\n",
    "            images, labels = data\n",
    "            #images = images.to(device).half() # uncomment for half precision model\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_acc = 100.0 * correct / total\n",
    "    \n",
    "    msg = f'Test accuracy: {test_acc:.4f}%'\n",
    "    logging.info(msg)\n",
    "    \n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583bed5f-1f83-4da8-b469-764a971512ae",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8fc5de-354b-4546-bdf3-0d8642c1b4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_dl, test_dl = get_dataloaders(DATASET_DIR)\n",
    "    \n",
    "    # I use the wonderful timm package to load the pretrained efficientnet model\n",
    "    # https://github.com/rwightman/pytorch-image-models\n",
    "    model_ft = timm.create_model('efficientnet_b3', pretrained=True)\n",
    "    \n",
    "    # replace the last fc layer with an untrained one (requires grad by default)\n",
    "    num_ftrs = model_ft.classifier.in_features\n",
    "    model_ft.classifier = nn.Linear(num_ftrs, NUM_CAR_CLASSES)\n",
    "    model_ft.to(device)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "    \n",
    "    lrscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       mode='max',\n",
    "                                                       patience=3,\n",
    "                                                       threshold = 0.9,\n",
    "                                                       min_lr=1e-6,\n",
    "                                                       verbose=True,\n",
    "                                                      )\n",
    "    \n",
    "    # Use this format (%Y-%m-%dT%H:%M:%SZ) to record timestamp of the metrics.\n",
    "    # If log_path is empty print log to StdOut, otherwise print log to the file.\n",
    "    if LOG_PATH == \"\":\n",
    "        logging.basicConfig(\n",
    "            format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "            datefmt=\"%Y-%m-%dT%H:%M:%SZ\",\n",
    "            level=logging.INFO)\n",
    "    else:\n",
    "        logging.basicConfig(\n",
    "            format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "            datefmt=\"%Y-%m-%dT%H:%M:%SZ\",\n",
    "            level=logging.INFO,\n",
    "            filename=LOG_PATH)\n",
    "    \n",
    "    model_ft, training_losses, training_accs, test_accs = train_model(model_ft,\n",
    "                                                                      train_dl,\n",
    "                                                                      loss_fn,\n",
    "                                                                      optimizer,\n",
    "                                                                      lrscheduler,\n",
    "                                                                      n_epochs=20\n",
    "                                                                     )\n",
    "    \n",
    "    # plot the stats\n",
    "\n",
    "    f, axarr = plt.subplots(2,2, figsize = (12, 8))\n",
    "    axarr[0, 0].plot(training_losses)\n",
    "    axarr[0, 0].set_title(\"Training loss\")\n",
    "    axarr[0, 1].plot(training_accs)\n",
    "    axarr[0, 1].set_title(\"Training acc\")\n",
    "    axarr[1, 0].plot(test_accs)\n",
    "\n",
    "    axarr[1, 0].set_title(\"Test acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e13ec6-f94a-45c4-9b0b-829433025207",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437826cc-ff9c-4fb4-a41d-e20ec4a93c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the stats\n",
    "\n",
    "f, axarr = plt.subplots(2,2, figsize = (12, 8))\n",
    "axarr[0, 0].plot(training_losses)\n",
    "axarr[0, 0].set_title(\"Training loss\")\n",
    "axarr[0, 1].plot(training_accs)\n",
    "axarr[0, 1].set_title(\"Training acc\")\n",
    "axarr[1, 0].plot(test_accs)\n",
    "\n",
    "axarr[1, 0].set_title(\"Test acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b7728-416e-4d4f-a4f1-615709ce8ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
